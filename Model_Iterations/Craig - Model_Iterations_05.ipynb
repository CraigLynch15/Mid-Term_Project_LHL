{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2c9b6d",
   "metadata": {},
   "source": [
    "# Optimizing Model - Parameter Optimization for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8abd41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0515bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flight_info = pd.read_csv('/Users/craiglynch/Desktop/Lighthouse_Labs/Mid-term_Project/mid-term-project-I-master/data_for_model_iterations_03.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3da1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather = pd.read_csv('/Users/craiglynch/Desktop/Lighthouse_Labs/Mid-term_Project/mid-term-project-I-master/weather.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c279094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight_info = data_flight_info.copy()\n",
    "df_weather = data_weather.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd9ec3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sunny     5503\n",
       "cloudy    4324\n",
       "rain      2860\n",
       "snow      1412\n",
       "Name: weather_type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather['value'] = df_weather['value'].str.lower()\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (df_weather['value'].str.contains('sunny')),\n",
    "    (df_weather['value'].str.contains('cloud')),\n",
    "    (df_weather['value'].str.contains('mist')),\n",
    "    (df_weather['value'].str.contains('fog')),\n",
    "    (df_weather['value'].str.contains('overcast')),\n",
    "    (df_weather['value'].str.contains('rain')),\n",
    "    (df_weather['value'].str.contains('drizzle')),\n",
    "    (df_weather['value'].str.contains('thunder')),\n",
    "    (df_weather['value'].str.contains('snow')),\n",
    "    (df_weather['value'].str.contains('sleet')),\n",
    "    (df_weather['value'].str.contains('blizzard')),\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['sunny', 'cloudy', 'cloudy','cloudy','cloudy','rain','rain','rain','snow','snow','snow']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_weather['weather_type'] = np.select(conditions, values)\n",
    "df_weather['weather_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bce729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14099"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather['value'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f53ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating year, month, date columns\n",
    "df_weather['year'] = pd.DatetimeIndex(df_weather['date']).year\n",
    "df_weather['month'] = pd.DatetimeIndex(df_weather['date']).month\n",
    "df_weather['day'] = pd.DatetimeIndex(df_weather['date']).day\n",
    "df_weather['weekday'] = ((pd.DatetimeIndex(df_weather['date']).dayofweek))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc32db9",
   "metadata": {},
   "source": [
    "#### Merging weather data with flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305b8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.rename(columns={'date': 'fl_date', 'airport_code': 'origin'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3be7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight_info = pd.merge(df_flight_info, df_weather[['fl_date','origin','weather_type']],  how='left', left_on=['fl_date','origin'], right_on = ['fl_date','origin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48a3734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>dest</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_delay</th>\n",
       "      <th>nas_delay</th>\n",
       "      <th>security_delay</th>\n",
       "      <th>late_aircraft_delay</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>total_carrier_delay</th>\n",
       "      <th>weather_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>169</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>LAS</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.841662</td>\n",
       "      <td>sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>169</td>\n",
       "      <td>IAH</td>\n",
       "      <td>1431.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>LAS</td>\n",
       "      <td>1531.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.841662</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>128</td>\n",
       "      <td>LAX</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>BWI</td>\n",
       "      <td>444.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.841662</td>\n",
       "      <td>sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>616</td>\n",
       "      <td>BOS</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>1822</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MSP</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.841662</td>\n",
       "      <td>sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>310</td>\n",
       "      <td>LAX</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>2.0</td>\n",
       "      <td>FLL</td>\n",
       "      <td>443.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.841662</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_date mkt_unique_carrier op_unique_carrier  op_carrier_fl_num origin  \\\n",
       "0  2018-10-19                 NK                NK                169    IAH   \n",
       "1  2018-09-24                 NK                NK                169    IAH   \n",
       "2  2018-03-21                 NK                NK                128    LAX   \n",
       "3  2018-04-24                 NK                NK                616    BOS   \n",
       "4  2018-04-22                 NK                NK                310    LAX   \n",
       "\n",
       "   dep_time  crs_dep_time  dep_delay dest  arr_time  ...  weather_delay  \\\n",
       "0    1433.0          1435       -2.0  LAS    1552.0  ...            0.0   \n",
       "1    1431.0          1435       -4.0  LAS    1531.0  ...            0.0   \n",
       "2    2051.0          2100       -9.0  BWI     444.0  ...            0.0   \n",
       "3    1824.0          1822        2.0  MSP    2037.0  ...            0.0   \n",
       "4    2107.0          2105        2.0  FLL     443.0  ...            0.0   \n",
       "\n",
       "   nas_delay  security_delay  late_aircraft_delay  year  month  day  weekday  \\\n",
       "0        0.0             0.0                  0.0  2018     10   19      0.0   \n",
       "1        0.0             0.0                  0.0  2018      9   24      0.0   \n",
       "2        0.0             0.0                  0.0  2018      3   21      0.0   \n",
       "3        0.0             0.0                  0.0  2018      4   24      0.0   \n",
       "4        0.0             0.0                  0.0  2018      4   22      1.0   \n",
       "\n",
       "   total_carrier_delay  weather_type  \n",
       "0             5.841662         sunny  \n",
       "1             5.841662          rain  \n",
       "2             5.841662         sunny  \n",
       "3             5.841662         sunny  \n",
       "4             5.841662        cloudy  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e5b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average route times\n",
    "df_flight_info['flight_number'] = df_flight_info['op_unique_carrier'] + df_flight_info['op_carrier_fl_num'].astype(str)\n",
    "route_time_grouped = df_flight_info.groupby(by='flight_number').mean()\n",
    "route_time_grouped.reset_index(inplace=True)\n",
    "route_time_grouped['average_route_time'] = route_time_grouped['actual_elapsed_time']\n",
    "route_time_grouped = route_time_grouped[['flight_number','average_route_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11055f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Average Weather Delay\n",
    "weather_grouped1 = df_flight_info.groupby(by=['origin','month']).mean()\n",
    "weather_grouped1.reset_index(inplace=True)\n",
    "weather_grouped1['total_weather_delay'] = weather_grouped1['weather_delay']\n",
    "weather_grouped1 = weather_grouped1[['origin','total_weather_delay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d72a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Average NAS Delay - Grouped by Flight Number\n",
    "nas_grouped = df_flight_info.groupby(by='flight_number').mean()\n",
    "nas_grouped.reset_index(inplace=True)\n",
    "nas_grouped['total_nas_delay'] = nas_grouped['nas_delay']\n",
    "nas_grouped = nas_grouped[['flight_number', 'total_nas_delay']]\n",
    "\n",
    "# Getting Average Security Delay\n",
    "security_grouped = df_flight_info.groupby(by='origin').mean()\n",
    "security_grouped.reset_index(inplace=True)\n",
    "security_grouped['total_security_delay'] = security_grouped['security_delay']\n",
    "security_grouped = security_grouped[['origin','total_security_delay']]\n",
    "\n",
    "# Getting Average Departure Delays by Route\n",
    "dep_delay_grouped = df_flight_info.groupby(by='flight_number').mean()\n",
    "dep_delay_grouped.reset_index(inplace=True)\n",
    "dep_delay_grouped['total_dep_delay'] = dep_delay_grouped['dep_delay']\n",
    "dep_delay_grouped = dep_delay_grouped[['flight_number','total_dep_delay']]\n",
    "\n",
    "# Getting Average Arrival Delays by Route\n",
    "arr_delay_grouped = df_flight_info.groupby(by='flight_number').mean()\n",
    "arr_delay_grouped.reset_index(inplace=True)\n",
    "arr_delay_grouped['total_arr_delay'] = arr_delay_grouped['arr_delay']\n",
    "arr_delay_grouped = arr_delay_grouped[['flight_number','total_arr_delay']]\n",
    "\n",
    "#Merge averages dataframes with original \n",
    "df_flight_info = df_flight_info.merge(security_grouped, on = 'origin', how = 'outer')\n",
    "df_flight_info = df_flight_info.merge(nas_grouped, on = 'flight_number', how = 'outer')\n",
    "df_flight_info = df_flight_info.merge(route_time_grouped, on = 'flight_number', how = 'outer')\n",
    "df_flight_info = df_flight_info.merge(dep_delay_grouped, on = 'flight_number', how = 'outer')\n",
    "df_flight_info = df_flight_info.merge(arr_delay_grouped, on = 'flight_number', how = 'outer')\n",
    "df_flight_info = df_flight_info.merge(weather_grouped1, on = 'origin', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54957203",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e2d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_flight_info.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e48ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, r2_score, f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe270a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding df_flight_info \n",
    "df_flight_info[\"weather_type\"] = df_flight_info[\"weather_type\"].astype('category').cat.codes\n",
    "df_flight_info[\"flight_number\"] = df_flight_info[\"flight_number\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60ffd910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fl_date                  object\n",
       "mkt_unique_carrier       object\n",
       "op_unique_carrier        object\n",
       "op_carrier_fl_num         int64\n",
       "origin                   object\n",
       "dep_time                float64\n",
       "crs_dep_time              int64\n",
       "dep_delay               float64\n",
       "dest                     object\n",
       "arr_time                float64\n",
       "arr_delay               float64\n",
       "crs_arr_time              int64\n",
       "actual_elapsed_time     float64\n",
       "crs_elapsed_time        float64\n",
       "distance                float64\n",
       "carrier_delay           float64\n",
       "weather_delay           float64\n",
       "nas_delay               float64\n",
       "security_delay          float64\n",
       "late_aircraft_delay     float64\n",
       "year                      int64\n",
       "month                     int64\n",
       "day                       int64\n",
       "weekday                 float64\n",
       "total_carrier_delay     float64\n",
       "weather_type               int8\n",
       "flight_number             int16\n",
       "total_security_delay    float64\n",
       "total_nas_delay         float64\n",
       "average_route_time      float64\n",
       "total_dep_delay         float64\n",
       "total_arr_delay         float64\n",
       "total_weather_delay     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight_info.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25d4bd",
   "metadata": {},
   "source": [
    "### Adding Total Weather Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e73c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight_info = shuffle(df_flight_info)\n",
    "X = df_flight_info[['crs_dep_time','flight_number','crs_elapsed_time','average_route_time','crs_arr_time','distance','year','month','day','weekday','total_carrier_delay', 'total_security_delay','total_nas_delay','total_dep_delay','total_arr_delay','weather_type','total_weather_delay']]\n",
    "y = df_flight_info['arr_delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7083582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957633ec",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c6ad491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6513  \u001b[0m | \u001b[0m 0.9895  \u001b[0m | \u001b[0m 0.2812  \u001b[0m | \u001b[0m 0.5985  \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 24.1    \u001b[0m | \u001b[0m 20.17   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 74.94   \u001b[0m | \u001b[0m 0.4615  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9895264513703341, subsample=0.4615201756036703 will be ignored. Current value: bagging_fraction=0.9895264513703341\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 192\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6546  \u001b[0m | \u001b[95m 0.9964  \u001b[0m | \u001b[95m 0.7939  \u001b[0m | \u001b[95m 0.9862  \u001b[0m | \u001b[95m 84.63   \u001b[0m | \u001b[95m 12.59   \u001b[0m | \u001b[95m 70.77   \u001b[0m | \u001b[95m 12.12   \u001b[0m | \u001b[95m 67.99   \u001b[0m | \u001b[95m 0.258   \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9963605418316657, subsample=0.2580000042232922 will be ignored. Current value: bagging_fraction=0.9963605418316657\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 373\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 373\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 373\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6594  \u001b[0m | \u001b[95m 0.8192  \u001b[0m | \u001b[95m 0.8548  \u001b[0m | \u001b[95m 0.8278  \u001b[0m | \u001b[95m 56.28   \u001b[0m | \u001b[95m 26.84   \u001b[0m | \u001b[95m 54.7    \u001b[0m | \u001b[95m 45.01   \u001b[0m | \u001b[95m 62.09   \u001b[0m | \u001b[95m 0.4252  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8192059420307759, subsample=0.4252313651032907 will be ignored. Current value: bagging_fraction=0.8192059420307759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6366  \u001b[0m | \u001b[0m 0.9281  \u001b[0m | \u001b[0m 0.5869  \u001b[0m | \u001b[0m 0.1144  \u001b[0m | \u001b[0m 87.62   \u001b[0m | \u001b[0m 23.97   \u001b[0m | \u001b[0m 60.78   \u001b[0m | \u001b[0m 32.93   \u001b[0m | \u001b[0m 25.48   \u001b[0m | \u001b[0m 0.8056  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9280786928822388, subsample=0.805623782185316 will be ignored. Current value: bagging_fraction=0.9280786928822388\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 149\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 149\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 149\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6431  \u001b[0m | \u001b[0m 0.9946  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.6526  \u001b[0m | \u001b[0m 38.59   \u001b[0m | \u001b[0m 9.692   \u001b[0m | \u001b[0m 45.14   \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 52.98   \u001b[0m | \u001b[0m 0.8559  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.994565837939734, subsample=0.8559475937048635 will be ignored. Current value: bagging_fraction=0.994565837939734\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6485  \u001b[0m | \u001b[0m 0.8518  \u001b[0m | \u001b[0m 0.7594  \u001b[0m | \u001b[0m 0.1626  \u001b[0m | \u001b[0m 54.26   \u001b[0m | \u001b[0m 24.77   \u001b[0m | \u001b[0m 51.27   \u001b[0m | \u001b[0m 45.05   \u001b[0m | \u001b[0m 63.4    \u001b[0m | \u001b[0m 0.9023  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.851823068170809, subsample=0.9023447381044342 will be ignored. Current value: bagging_fraction=0.851823068170809\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 218\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.651   \u001b[0m | \u001b[0m 0.9094  \u001b[0m | \u001b[0m 0.4572  \u001b[0m | \u001b[0m 0.8601  \u001b[0m | \u001b[0m 85.95   \u001b[0m | \u001b[0m 14.68   \u001b[0m | \u001b[0m 54.13   \u001b[0m | \u001b[0m 43.72   \u001b[0m | \u001b[0m 59.64   \u001b[0m | \u001b[0m 0.8433  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9093814222077197, subsample=0.8433313512183651 will be ignored. Current value: bagging_fraction=0.9093814222077197\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 205\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 205\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 205\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6516  \u001b[0m | \u001b[0m 0.9741  \u001b[0m | \u001b[0m 0.6136  \u001b[0m | \u001b[0m 0.4543  \u001b[0m | \u001b[0m 22.1    \u001b[0m | \u001b[0m 13.52   \u001b[0m | \u001b[0m 52.09   \u001b[0m | \u001b[0m 2.789   \u001b[0m | \u001b[0m 49.11   \u001b[0m | \u001b[0m 0.94    \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9740935568038949, subsample=0.940008263572769 will be ignored. Current value: bagging_fraction=0.9740935568038949\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6536  \u001b[0m | \u001b[0m 0.9003  \u001b[0m | \u001b[0m 0.7149  \u001b[0m | \u001b[0m 0.9819  \u001b[0m | \u001b[0m 57.47   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 44.02   \u001b[0m | \u001b[0m 60.93   \u001b[0m | \u001b[0m 47.88   \u001b[0m | \u001b[0m 0.5404  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.900260971230219, subsample=0.5404491615142168 will be ignored. Current value: bagging_fraction=0.900260971230219\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 282\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 282\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 282\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6573  \u001b[0m | \u001b[0m 0.9023  \u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 0.4663  \u001b[0m | \u001b[0m 59.03   \u001b[0m | \u001b[0m 20.47   \u001b[0m | \u001b[0m 48.44   \u001b[0m | \u001b[0m 94.19   \u001b[0m | \u001b[0m 61.48   \u001b[0m | \u001b[0m 0.5169  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9022534679331193, subsample=0.5168969653573697 will be ignored. Current value: bagging_fraction=0.9022534679331193\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 409\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 409\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 409\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6398  \u001b[0m | \u001b[0m 0.8722  \u001b[0m | \u001b[0m 0.539   \u001b[0m | \u001b[0m 0.1459  \u001b[0m | \u001b[0m 75.5    \u001b[0m | \u001b[0m 29.72   \u001b[0m | \u001b[0m 68.65   \u001b[0m | \u001b[0m 31.57   \u001b[0m | \u001b[0m 33.03   \u001b[0m | \u001b[0m 0.07075 \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8722154028274957, subsample=0.07075367348204263 will be ignored. Current value: bagging_fraction=0.8722154028274957\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 270\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6559  \u001b[0m | \u001b[0m 0.8387  \u001b[0m | \u001b[0m 0.6125  \u001b[0m | \u001b[0m 0.7672  \u001b[0m | \u001b[0m 20.36   \u001b[0m | \u001b[0m 18.62   \u001b[0m | \u001b[0m 63.31   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 64.31   \u001b[0m | \u001b[0m 0.8977  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8387086103113183, subsample=0.8977100601260881 will be ignored. Current value: bagging_fraction=0.8387086103113183\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6547  \u001b[0m | \u001b[0m 0.9836  \u001b[0m | \u001b[0m 0.6629  \u001b[0m | \u001b[0m 0.8244  \u001b[0m | \u001b[0m 24.69   \u001b[0m | \u001b[0m 18.41   \u001b[0m | \u001b[0m 31.14   \u001b[0m | \u001b[0m 71.4    \u001b[0m | \u001b[0m 48.15   \u001b[0m | \u001b[0m 0.1012  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9836063675867542, subsample=0.1011988223298474 will be ignored. Current value: bagging_fraction=0.9836063675867542\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088474 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 94\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6357  \u001b[0m | \u001b[0m 0.9012  \u001b[0m | \u001b[0m 0.4162  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 38.58   \u001b[0m | \u001b[0m 6.34    \u001b[0m | \u001b[0m 48.05   \u001b[0m | \u001b[0m 18.3    \u001b[0m | \u001b[0m 60.35   \u001b[0m | \u001b[0m 0.1039  \u001b[0m |\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9011700977910886, subsample=0.10391082521944398 will be ignored. Current value: bagging_fraction=0.9011700977910886\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781566, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 244\n",
      "[LightGBM] [Info] Number of data points in the train set: 5781567, number of used features: 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668586565574406, subsample=0.9452065967526455 will be ignored. Current value: bagging_fraction=0.9668586565574406\n",
      "[LightGBM] [Info] Start training from score 5.144891\n",
      "[LightGBM] [Info] Start training from score 5.144833\n",
      "[LightGBM] [Info] Start training from score 5.144930\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6207  \u001b[0m | \u001b[0m 0.9669  \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 0.02344 \u001b[0m | \u001b[0m 24.41   \u001b[0m | \u001b[0m 17.37   \u001b[0m | \u001b[0m 21.12   \u001b[0m | \u001b[0m 84.04   \u001b[0m | \u001b[0m 57.78   \u001b[0m | \u001b[0m 0.9452  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt  import BayesSearchCV\n",
    "def bayes_parameter_opt_lgb(X_train, y_train, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
    "        params = {'application':'regression', 'metric':'rmse'}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        \n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "     \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
    "                                            'num_leaves': (24, 80),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (5, 30),\n",
    "                                            'max_bin':(20,90),\n",
    "                                            'min_data_in_leaf': (20, 80),\n",
    "                                            'min_sum_hessian_in_leaf':(0,100),\n",
    "                                           'subsample': (0.01, 1.0)}, random_state=200)\n",
    "\n",
    "    \n",
    "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
    "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
    "    \n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_auc=[]\n",
    "    for model in range(len( lgbBO.res)):\n",
    "        model_auc.append(lgbBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a009baf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-71d33d6ef8ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_leaves\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_leaves\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_data_in_leaf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_data_in_leaf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_bin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objective'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
    "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
    "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
    "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
    "opt_params[1]['objective']='binary'\n",
    "opt_params[1]['metric']='rmse'\n",
    "opt_params[1]['is_unbalance']=True\n",
    "opt_params[1]['boost_from_average']=False\n",
    "opt_params=opt_params[1]\n",
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b1173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6ed3ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2737\n",
      "[LightGBM] [Info] Number of data points in the train set: 8672350, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 5.144885\n",
      "Model RMSE is:  39.742361203280275\n",
      "Model R2 Score is:  0.16815526421564875\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.8277896457253541\n",
    "params['boosting_type'] = 'gbdt'\n",
    "#params['boosting_type'] = 'dart'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'rmse'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 62\n",
    "params['min_data'] = 55\n",
    "params['max_depth'] = 27\n",
    "y_train=y_train.ravel()\n",
    "reg= lgb.train(params, d_train, 100)\n",
    "results=reg.predict(X_test)\n",
    "print('Model RMSE is: ',np.sqrt(mean_squared_error(y_test, results)))\n",
    "print('Model R2 Score is: ',r2_score(y_test,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51951e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: 8277896457253541\n",
      "[LightGBM] [Warning] Unknown parameter: slient\n",
      "[LightGBM] [Warning] Unknown parameter: booster\n",
      "[LightGBM] [Warning] Unknown parameter: 0.8\n",
      "[LightGBM] [Warning] Unknown parameter: 8277896457253541\n",
      "[LightGBM] [Warning] Unknown parameter: slient\n",
      "[LightGBM] [Warning] Unknown parameter: booster\n",
      "[LightGBM] [Warning] Unknown parameter: 0.8\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2738\n",
      "[LightGBM] [Info] Number of data points in the train set: 8672350, number of used features: 17\n",
      "[LightGBM] [Warning] Unknown parameter: 8277896457253541\n",
      "[LightGBM] [Warning] Unknown parameter: slient\n",
      "[LightGBM] [Warning] Unknown parameter: booster\n",
      "[LightGBM] [Warning] Unknown parameter: 0.8\n",
      "[LightGBM] [Info] Start training from score 5.144885\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 62,\n",
    "    'subsample': 0.5,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction ': 0.8,\n",
    "    'slient': 1,\n",
    "    'learning_rate ': 8277896457253541,\n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Construct training set\n",
    "dtrain = lgb.Dataset(X_train,y_train)\n",
    "dtest = lgb.Dataset(X_test,y_test)\n",
    "num_rounds = 500\n",
    "# xgboost model training\n",
    "model = lgb.train(params,dtrain, num_rounds, valid_sets=[dtrain, dtest], \n",
    "                  verbose_eval=100, early_stopping_rounds=100)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print('Model RMSE is: ',np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('Model R2 Score is: ',r2_score(y_test,y_pred))\n",
    "\n",
    "# Show important features\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5076967",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a180ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE is:  42.74958892660707\n",
      "Model R2 Score is:  0.039126530436728646\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xg_reg.fit(X_test,y_test)\n",
    "\n",
    "y_pred_xgb = xg_reg.predict(X_test)\n",
    "\n",
    "print('Model RMSE is: ',np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
    "print('Model R2 Score is: ',r2_score(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3898edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Examine the importance of each feature column in the original data set with the model\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54072a9d",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Regressor\n",
    "clf=RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model using the training sets \n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model RMSE is: ',np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('Model R2 Score is: ',r2_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LighthouseLabs",
   "language": "python",
   "name": "lighthouselabs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
